{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1LOuuQaT4PTp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 10:55:50.603243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LZhG3304cwg",
    "outputId": "40456e83-0e81-48d9-b1ba-a5ca6935d2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7gmaZiwe5qMY"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open('glove.6B.50d.txt') as f:\n",
    "  for line in f:\n",
    "    word, coefs = line.split(maxsplit=1)\n",
    "    coefs = np.fromstring(coefs, \"f\", sep = ' ')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wwGlpln6THS",
    "outputId": "2fa37aec-2f8c-4ee7-f2e0-e53dbb8c0cca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30824 ,  0.17223 , -0.23339 ,  0.023105,  0.28522 ,  0.23076 ,\n",
       "       -0.41048 , -1.0035  , -0.2072  ,  1.4327  , -0.80684 ,  0.68954 ,\n",
       "       -0.43648 ,  1.1069  ,  1.6107  , -0.31966 ,  0.47744 ,  0.79395 ,\n",
       "       -0.84374 ,  0.064509,  0.90251 ,  0.78609 ,  0.29699 ,  0.76057 ,\n",
       "        0.433   , -1.5032  , -1.6423  ,  0.30256 ,  0.30771 , -0.87057 ,\n",
       "        2.4782  , -0.025852,  0.5013  , -0.38593 , -0.15633 ,  0.45522 ,\n",
       "        0.04901 , -0.42599 , -0.86402 , -1.3076  , -0.29576 ,  1.209   ,\n",
       "       -0.3127  , -0.72462 , -0.80801 ,  0.082667,  0.26738 , -0.98177 ,\n",
       "       -0.32147 ,  0.99823 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index ['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snL-KTLE6eN3"
   },
   "source": [
    "# Get the movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LFLlyur6hnS",
    "outputId": "c4e80a1b-951e-4a54-ce08-426bf554b867"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000, maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8Itbnb6262XS"
   },
   "outputs": [],
   "source": [
    "x_all = np.append(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NQFseWjD66vH"
   },
   "outputs": [],
   "source": [
    "x_all_padded = tf.keras.utils.pad_sequences(x_all, padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ipdIoHC77DTb"
   },
   "outputs": [],
   "source": [
    "word_lookup = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gjPVJZOw7pjX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lookup['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gRNBiuuD7tpT"
   },
   "outputs": [],
   "source": [
    "inverted_word_lookup = dict((index +3, word) for (word, index) in word_lookup.items())\n",
    "\n",
    "inverted_word_lookup[0] = \"[PAD]\"\n",
    "inverted_word_lookup[1] = \"[START]\"\n",
    "inverted_word_lookup[2] = \"[OOV]\"\n",
    "inverted_word_lookup[3] = \"[NA]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    return \"\".join(inverted_word_lookup[i] for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[START]theviewerleaveswonderingwhyhebotheredtowatchthisoneorwhyforthatmatteranyonebotheredtomakeitthereisnoplotjustrandomscenesofridiculousaction[OOV][OOV]showersceneappealstothemale[OOV]butthat'snotmuchreasontomakeamovie\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(x_all[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "else's\n",
      "miyazaki's\n",
      "victoria's\n",
      "paul's\n",
      "chan's\n",
      "show's\n",
      "wife's\n",
      "character's\n",
      "hadn't\n",
      "isn't\n",
      "haven't\n",
      "wouldn't\n",
      "its'\n",
      "she'd\n",
      "she's\n",
      "paperhouse\n",
      "they'll\n",
      "it's\n",
      "it'd\n",
      "daughter's\n",
      "ted's\n",
      "ben's\n",
      "america's\n",
      "men's\n",
      "he'll\n",
      "john's\n",
      "audience's\n",
      "30's\n",
      "mom's\n",
      "hero's\n",
      "hasn't\n",
      "should've\n",
      "imho\n",
      "keaton's\n",
      "they'd\n",
      "zelah\n",
      "you'll\n",
      "smith's\n",
      "girls'\n",
      "craven's\n",
      "feinstone\n",
      "moore's\n",
      "eastwood's\n",
      "kids'\n",
      "tv's\n",
      "town's\n",
      "anyone's\n",
      "writer's\n",
      "1960's\n",
      "kubrick's\n",
      "husband's\n",
      "allen's\n",
      "80's\n",
      "stewart's\n",
      "t'aime\n",
      "boy's\n",
      "man'\n",
      "scott's\n",
      "it´s\n",
      "bakshi's\n",
      "",
      "\n",
      "\n",
      "person's\n",
      "you've\n",
      "verhoeven's\n",
      "spielberg's\n",
      "it'll\n",
      "carpenter's\n",
      "life's\n",
      "sister's\n",
      "family's\n",
      "who've\n",
      "director's\n",
      "where's\n",
      "city's\n",
      "author's\n",
      "man's\n",
      "friend's\n",
      "we'd\n",
      "would've\n",
      "day's\n",
      "freddy's\n",
      "woman's\n",
      "1930's\n",
      "can't\n",
      "ain't\n",
      "actors'\n",
      "90's\n",
      "ossessione\n",
      "ford's\n",
      "couldn't\n",
      "1990's\n",
      "won't\n",
      "that'll\n",
      "other's\n",
      "aren't\n",
      "doctor's\n",
      "everybody's\n",
      "jackson's\n",
      "we're\n",
      "hollywood's\n",
      "kelly's\n",
      "david's\n",
      "murphy's\n",
      "dvd's\n",
      "shakespeare's\n",
      "characters'\n",
      "mother's\n",
      "he's\n",
      "he'd\n",
      "hitler's\n",
      "everyone's\n",
      "don't\n",
      "could've\n",
      "child's\n",
      "miike's\n",
      "simon's\n",
      "children's\n",
      "let's\n",
      "didn't\n",
      "you're\n",
      "bug's\n",
      "40's\n",
      "someone's\n",
      "today's\n",
      "gypo\n",
      "lynch's\n",
      "1950's\n",
      "palma's\n",
      "michael's\n",
      "girl's\n",
      "killer's\n",
      "50's\n",
      "charlie's\n",
      "you'd\n",
      "doesn't\n",
      "story's\n",
      "i'd\n",
      "i'm\n",
      "they're\n",
      "what's\n",
      "devil's\n",
      "1980's\n",
      "rosemary's\n",
      "\n",
      "there's\n",
      "world's\n",
      "father's\n",
      "wasn't\n",
      "60's\n",
      "carface\n",
      "guy's\n",
      "hartley's\n",
      "wayne's\n",
      "1970's\n",
      "sinatra's\n",
      "year's\n",
      "god's\n",
      "who's\n",
      "who'd\n",
      "'a\n",
      "'i\n",
      "film's\n",
      "romero's\n",
      "actor's\n",
      "tony's\n",
      "must've\n",
      "jack's\n",
      "people's\n",
      "lee's\n",
      "here's\n",
      "that's\n",
      "kid's\n",
      "jones'\n",
      "welles'\n",
      "shouldn't\n",
      "'the\n",
      "one's\n",
      "carlito's\n",
      "che's\n",
      "1940's\n",
      "70's\n",
      "altman's\n",
      "we've\n",
      "son's\n",
      "they've\n",
      "king's\n",
      "jane's\n",
      "we'll\n",
      "disney's\n",
      "hitchcock's\n",
      "she'll\n",
      "country's\n",
      "i'll\n",
      "harry's\n",
      "movie's\n",
      "weren't\n",
      "women's\n",
      "brother's\n",
      "viewer's\n",
      "branagh's\n",
      "parents'\n",
      "i've\n",
      "[PAD]\n",
      "[START]\n",
      "[OOV]\n",
      "[NA]\n",
      "9797 hits, 207 misses\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((10004, 50))\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "for i, word in inverted_word_lookup.items():\n",
    "    if (i >= 10004):\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        print(word)\n",
    "\n",
    "print(\"%d hits, %d misses\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a decoder-only transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)       [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)    (None, None, 50)             500200    ['input_12[0][0]']            \n",
      "                                                                                                  \n",
      " sine_position_encoding_10   (None, None, 50)             0         ['embedding_11[0][0]']        \n",
      " (SinePositionEncoding)                                                                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TF  (None, None, 50)             0         ['embedding_11[0][0]',        \n",
      " OpLambda)                                                           'sine_position_encoding_10[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, None, 50)             101550    ['tf.__operators__.add_8[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'tf.__operators__.add_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, None, 50)             0         ['tf.__operators__.add_8[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, None, 50)             100       ['add_14[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_21 (Dense)            (None, None, 50)             2550      ['layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, None, 50)             0         ['layer_normalization_14[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'dense_21[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, None, 50)             100       ['add_15[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, None, 50)             2550      ['layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_23 (Dense)            (None, None, 10004)          510204    ['dense_22[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1117254 (4.26 MB)\n",
      "Trainable params: 617054 (2.35 MB)\n",
      "Non-trainable params: 500200 (1.91 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# WORD + POSITION EMBEDDING\n",
    "input_layer = tf.keras.layers.Input(shape=(None,))\n",
    "word_embedding = tf.keras.layers.Embedding(10004, 50, embeddings_initializer = tf.keras.initializers.constant(embedding_matrix), trainable=False, mask_zero=True)(input_layer)\n",
    "position_embedding = keras_nlp.layers.SinePositionEncoding()(word_embedding)\n",
    "word_and_position_embedding = word_embedding + position_embedding # taking output of these two layers and element wise adds them\n",
    "# ATTENTION\n",
    "attention = tf.keras.layers.MultiHeadAttention(10, 50)(word_and_position_embedding, word_and_position_embedding, use_causal_mask=True) # one for query, the other for key (keys and value the same so no need third), causal mask: since we give the model the entire sentence at a time, we want it to not have access to words in the future at a given position in a sentence. So we use this mask to mask out words ahead so the self attention cannot look ahead\n",
    "residual = tf.keras.layers.Add()([word_and_position_embedding, attention])\n",
    "normalize = tf.keras.layers.LayerNormalization()(residual)\n",
    "# DENSE\n",
    "dense = tf.keras.layers.Dense(50, activation=\"relu\")(normalize)\n",
    "residual_dense = tf.keras.layers.Add()([normalize, dense])\n",
    "normalize_dense = tf.keras.layers.LayerNormalization()(residual_dense)\n",
    "\n",
    "# OUTPUT\n",
    "linear = tf.keras.layers.Dense(50, activation=None)(normalize_dense)\n",
    "output_layer = tf.keras.layers.Dense(10004, activation=\"softmax\")(linear)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.roll(x_all_padded, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
